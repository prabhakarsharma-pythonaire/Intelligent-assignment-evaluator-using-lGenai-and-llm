{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "\n",
    "# Specify the path to tesseract.exe\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip install pytesseract\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the image\n",
    "image = Image.open(Path('artifacts\\Input_images\\image_1.png'))\n",
    "\n",
    "# Extract text from the image\n",
    "text = pytesseract.image_to_string(image)\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__version__\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "print(cv2.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'imread'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m reader \u001b[38;5;241m=\u001b[39m easyocr\u001b[38;5;241m.\u001b[39mReader([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the image and extract text\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadtext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43martifacts\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mInput_images\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mimage_1.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Print only the extracted text\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (bbox, text, prob) \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[1;32mc:\\Users\\ayush\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\easyocr\\easyocr.py:454\u001b[0m, in \u001b[0;36mReader.readtext\u001b[1;34m(self, image, decoder, beamWidth, batch_size, workers, allowlist, blocklist, detail, rotation_info, paragraph, min_size, contrast_ths, adjust_contrast, filter_ths, text_threshold, low_text, link_threshold, canvas_size, mag_ratio, slope_ths, ycenter_ths, height_ths, width_ths, y_ths, x_ths, add_margin, threshold, bbox_min_score, bbox_min_size, max_candidates, output_format)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadtext\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreedy\u001b[39m\u001b[38;5;124m'\u001b[39m, beamWidth\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\\\n\u001b[0;32m    441\u001b[0m              workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, allowlist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, blocklist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, detail \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\\\n\u001b[0;32m    442\u001b[0m              rotation_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, paragraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, min_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m,\\\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    448\u001b[0m              threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m, bbox_min_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m, bbox_min_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, max_candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    449\u001b[0m              output_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstandard\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    450\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;124;03m    Parameters:\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m    image: file path or numpy-array or a byte stream object\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m--> 454\u001b[0m     img, img_cv_grey \u001b[38;5;241m=\u001b[39m \u001b[43mreformat_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    456\u001b[0m     horizontal_list, free_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetect(img, \n\u001b[0;32m    457\u001b[0m                                              min_size \u001b[38;5;241m=\u001b[39m min_size, text_threshold \u001b[38;5;241m=\u001b[39m text_threshold,\\\n\u001b[0;32m    458\u001b[0m                                              low_text \u001b[38;5;241m=\u001b[39m low_text, link_threshold \u001b[38;5;241m=\u001b[39m link_threshold,\\\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    464\u001b[0m                                              bbox_min_size \u001b[38;5;241m=\u001b[39m bbox_min_size, max_candidates \u001b[38;5;241m=\u001b[39m max_candidates\n\u001b[0;32m    465\u001b[0m                                              )\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;66;03m# get the 1st result from hor & free list as self.detect returns a list of depth 3\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\easyocr\\utils.py:739\u001b[0m, in \u001b[0;36mreformat_input\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m    737\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(tmp)\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 739\u001b[0m     img_cv_grey \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m(image, cv2\u001b[38;5;241m.\u001b[39mIMREAD_GRAYSCALE)\n\u001b[0;32m    740\u001b[0m     image \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(image)\n\u001b[0;32m    741\u001b[0m img \u001b[38;5;241m=\u001b[39m loadImage(image)  \u001b[38;5;66;03m# can accept URL\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'imread'"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "\n",
    "# Initialize reader\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "# Load the image and extract text\n",
    "results = reader.readtext('artifacts\\Input_images\\image_1.png')\n",
    "\n",
    "# Print only the extracted text\n",
    "for (bbox, text, prob) in results:\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'imread'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m a\u001b[38;5;241m=\u001b[39m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124martifacts\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mInput_images\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mimage_1.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(a)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'imread'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "a=cv2.imread('artifacts\\Input_images\\image_1.png')\n",
    "cv2.imshow(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagu Ho; Date; CountA Udllus 4ne numbesy monz blank Qecends alala qelum \"drotn valULa y: a columo) lound the non- blanR 6T ob nurerycal reqardlw mumerica l COUNTA [<columz ] 4. . Snbax \". . : ' . ' :. ' ' .  . ' 1 . . ' \" ' ' CountX: 1a ued 40 cexant tn numbeg{ rela 43 4 table Lshese an exbreslon [eualuab to ' 0 non -blank vadu uoeful uhen +8U need 1e aeb come calculatlen or terdixon Refasje eountlg tno rou Kus . 1.'  : o ` . ` COUNTK [ < table > , < exEresmz ] . . Sfb t , ' 4. ' ` . COUNTBLA NK Is used + coltt the numlen bkank (or: missm Nalues lolumn uceful ~r cota swu &ixy checks and idettifrhq rysabx data withen clatasd : ^ . ' dowt ' ; Atak (OuNTPLANK [aelumn> DISTINCTCOUNT uSed 1 torud tne numbest blahk lalu In tolym useUl scenarios lshere Yey need Jo qdermin the numbes  Stems &uch c 'Youva unkcwue non- Ter uniale \n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "\n",
    "# Initialize reader\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "# Load the image and extract text\n",
    "results = reader.readtext('artifacts\\Input_images\\image_1.png')\n",
    "\n",
    "# Print results\n",
    "sentence = \"\"\n",
    "for (bbox, text, prob) in results:\n",
    "    sentence += text + \" \"\n",
    "\n",
    "print(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagu Ho; Date; CountA Udllus 4ne numbesy monz blank Qecends alala qelum \"drotn valULa y: a columo) lound the non- blanR 6T ob nurerycal reqardlw mumer\n",
      "ica l COUNTA [<columz ] 4. . Snbax \". . : ' . ' :. ' ' .  . ' 1 . . ' \" ' ' CountX: 1a ued 40 cexant tn numbeg{ rela 43 4 table Lshese an exbreslon [e\n",
      "ualuab to ' 0 non -blank vadu uoeful uhen +8U need 1e aeb come calculatlen or terdixon Refasje eountlg tno rou Kus . 1.'  : o ` . ` COUNTK [ < table >\n",
      " , < exEresmz ] . . Sfb t , ' 4. ' ` . COUNTBLA NK Is used + coltt the numlen bkank (or: missm Nalues lolumn uceful ~r cota swu &ixy checks and idetti\n",
      "frhq rysabx data withen clatasd : ^ . ' dowt ' ; Atak (OuNTPLANK [aelumn> DISTINCTCOUNT uSed 1 torud tne numbest blahk lalu In tolym useUl scenarios l\n",
      "shere Yey need Jo qdermin the numbes  Stems &uch c 'Youva unkcwue non- Ter uniale \n"
     ]
    }
   ],
   "source": [
    "def insert_line_breaks(text, n):\n",
    "    return '\\n'.join([text[i:i+n] for i in range(0, len(text), n)])\n",
    "\n",
    "# Insert line breaks after every 150 characters\n",
    "formatted_text = insert_line_breaks(sentence, 150)\n",
    "\n",
    "# Print the formatted text\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance: 1379\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "from pathlib import Path\n",
    "def calculate_levenshtein_distance(text1, text2):\n",
    "    distance = Levenshtein.distance(text1, text2)\n",
    "    return distance\n",
    "\n",
    "def read__(path:Path):\n",
    "    with open(path) as f:\n",
    "        a=f.read()\n",
    "    return a\n",
    "text1 = read__(Path('D:\\END_TO_END_MAJOR\\data\\IMG_20240724_221056_text.txt'))\n",
    "text2 = read__(Path('D:/END_TO_END_MAJOR/data/text.txt'))\n",
    "\n",
    "distance = calculate_levenshtein_distance(text1, text2)\n",
    "print(f\"Levenshtein Distance: {distance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in text1 but not in text2:\n",
      "{'averagex', 'ignoring', 'functions', 'time_value', 'analysing', 'updates', 'some', 'counting', 'adapt', 'end_date', 'fiscal_year_end_month', 'imports', 'assuming', 'default', 'start_date', 'want', 'known', 'starts'}\n",
      " 18\n",
      "Words in text2 but not in text1:\n",
      "{'analyzing', 'imported', 'into', '11', 'reports', 'particular', 'o', 'other', 'logical', '_', 'calculation', 'advance', 'interval', 'considered', 'condition', 'because', 'components', 'any', 'minuly', '2', 'boundaries', 'finding', 'dynamically', 'before', 'falls', 'stood', 'one'}\n",
      " 27\n",
      "Common words in both texts:\n",
      "{'business', 'in', 'fiscal', 'range', 'power', 'constructing', 'from', 'exact', 'sum', 'working', 'difference', 'unique', 'blanks', 'weeknum', 'up', 'hour', 'that', 'used', 'year', 'no', 'returns', 'specifying', 'm', 'a', 'quickly', 'event', 'import', 'data', 'high', 'calculations', 'apply', '4', 'excluded', 'need', 'w', 'handle', 'extract', 'component', 'calendar', 'helps', 'counts', 'is', 'week', 'mathematical', 'logging', 'for', 'converts', 'distribution', 'hours', 'part', '7', 'datediff', 'latest', 'today', 'model', 'useful', 'saturday', 'delivery', 'holidays', 'discount', 'optional', 'values', 'orderdate', 'determining', 'current', 'an', '12', 'analysis', 'across', 'mean', 'results', 'based', 'dates', 'single', 'such', 'set', 'different', 'create', 'within', 'weekends', 'numbers', '2024', 'maxx', 'require', 'without', 'quality', 'weekday', 'sales', 'calculating', 'flexibility', 'datetime', 'on', 'use', 'powerful', 'countblank', '06', '24', 'time', 'find', 'function', 'real', 'format', 'total', 'scenario', 'youva', 'types', 'when', '9999', 'terms', 'intervals', 'averagea', 's', 'built', 'as', 'precise', 'january', '30', 'extracting', 'june', 'max', 'quarter', 'but', 'customers', 'adapts', 'flexible', 'all', 'generates', 'arithmetic', 'countrows', 'each', 'since', 'essential', 'maximum', 'networkdays', 'seconds', 'null', 'it', 'monitoring', 'more', 'page', 'evaluates', 'adds', 'datevalue', 'generate', 'the', '35', 'string', 'extracts', 'or', 'between', 'excluding', 'products', 'calculates', 'f', 'counta', 'and', 'to', 'excludes', 'non', 'meaning', 'employee', 'over', 'end', 'distinctcount', 'timestamp', 'present', 'not', 'strings', 'summarizing', 'have', 'largest', 'bi', 'transactions', 'insights', '17', 'month', '31', 'daily', 'value', 'column', 'numeric', 'countx', 'csv', 'order', 'december', 'determine', 'treating', 'given', 'specification', 'holiday', 'entries', '6', 'support', 'timevalue', 'scenarios', 'can', 'weekend', 'blank', '20', 'further', 'rows', 'specified', '23', 'are', 'expression', 'performing', 'sale', 'iterator', 'minimum', 'calendarauto', 'two', 'months', 'numerical', 'be', 'case', 'refresh', 'whole', 'after', 'convert', 'detailed', 'transaction', 'aggregation', 'purposes', 'revenue', 'custom', 'dataset', 'sunday', 'dax', 'creating', 'your', 'minute', 'allows', 'checks', 'day', 'if', 'regardless', 'missing', 'automatically', 'operations', 'manipulating', 'adjusts', 'etc', '59', 'with', 'determines', '1', 'indicates', 'control', 'type', 'count', 'second', 'free', '14', 'specific', 'workdays', 'accordingly', 't', 'less', 'average', '0', 'which', 'return', 'years', 'corresponds', 'does', 'ending', 'columns', 'of', 'this', 'precision', 'text', 'both', 'form', 'quantity', 'meaningful', 'number', 'description', 'identifying', 'iterates', 'calculate', 'sumx', 'where', 'start', 'made', 'was', 'price', 'date', 'auto', 'then', 'adjust', 'syntax', 'table', 'now', '1900', 'days', 'deliverydate', 'minutes', 'example', 'including', 'converting', 'maxa', 'you', 'row'}\n",
      " 303\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define the texts\n",
    "\n",
    "\n",
    "# Function to clean and tokenize text\n",
    "def tokenize(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)  # Extract words\n",
    "    return set(tokens)  # Return unique words as a set\n",
    "\n",
    "# Tokenize the texts\n",
    "tokens1 = tokenize(text1)\n",
    "tokens2 = tokenize(text2)\n",
    "\n",
    "# Find unique and differing words\n",
    "unique_to_text1 = tokens1 - tokens2\n",
    "unique_to_text2 = tokens2 - tokens1\n",
    "common_words = tokens1 & tokens2\n",
    "\n",
    "print(f\"Words in text1 but not in text2:\\n{unique_to_text1}\\n {len(unique_to_text1)}\")\n",
    "print(f\"Words in text2 but not in text1:\\n{unique_to_text2}\\n {len(unique_to_text2)}\")\n",
    "print(f\"Common words in both texts:\\n{common_words}\\n {len(common_words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IMG_20240724_221056_text copy.txt</th>\n",
       "      <th>IMG_20240724_221056_text.txt</th>\n",
       "      <th>text copy 2.txt</th>\n",
       "      <th>text copy.txt</th>\n",
       "      <th>text.txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IMG_20240724_221056_text copy.txt</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986817</td>\n",
       "      <td>0.986817</td>\n",
       "      <td>0.986817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMG_20240724_221056_text.txt</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986817</td>\n",
       "      <td>0.986817</td>\n",
       "      <td>0.986817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text copy 2.txt</th>\n",
       "      <td>0.986817</td>\n",
       "      <td>0.986817</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text copy.txt</th>\n",
       "      <td>0.986817</td>\n",
       "      <td>0.986817</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text.txt</th>\n",
       "      <td>0.986817</td>\n",
       "      <td>0.986817</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   IMG_20240724_221056_text copy.txt  \\\n",
       "IMG_20240724_221056_text copy.txt                           1.000000   \n",
       "IMG_20240724_221056_text.txt                                1.000000   \n",
       "text copy 2.txt                                             0.986817   \n",
       "text copy.txt                                               0.986817   \n",
       "text.txt                                                    0.986817   \n",
       "\n",
       "                                   IMG_20240724_221056_text.txt  \\\n",
       "IMG_20240724_221056_text copy.txt                      1.000000   \n",
       "IMG_20240724_221056_text.txt                           1.000000   \n",
       "text copy 2.txt                                        0.986817   \n",
       "text copy.txt                                          0.986817   \n",
       "text.txt                                               0.986817   \n",
       "\n",
       "                                   text copy 2.txt  text copy.txt  text.txt  \n",
       "IMG_20240724_221056_text copy.txt         0.986817       0.986817  0.986817  \n",
       "IMG_20240724_221056_text.txt              0.986817       0.986817  0.986817  \n",
       "text copy 2.txt                           1.000000       1.000000  1.000000  \n",
       "text copy.txt                             1.000000       1.000000  1.000000  \n",
       "text.txt                                  1.000000       1.000000  1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the text files\n",
    "folder_path = 'D:\\END_TO_END_MAJOR\\data'  # Update this to your folder path\n",
    "files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.txt')]\n",
    "\n",
    "texts = []\n",
    "file_names = []\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        texts.append(f.read())\n",
    "        file_names.append(os.path.basename(file))\n",
    "\n",
    "# Step 2: Vectorize the text\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Step 3: Calculate similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Step 4: Create a DataFrame for better visualization\n",
    "similarity_df = pd.DataFrame(cosine_sim, index=file_names, columns=file_names)\n",
    "\n",
    "similarity_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
